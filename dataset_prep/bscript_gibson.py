import bpy
import math
from mathutils import Vector
import csv
import os
import json
from sklearn.cluster import KMeans, DBSCAN
import numpy as np
import sys
import shutil

# these are for single scene generation and debugging, use params to pass dataset location
render_size = 1024
floor_count = 4 
glb_location = r""
csv_location = r""
config_location = r""
output_location = "_"

def get_param(param_name):
    for arg in sys.argv:
        if arg.startswith(f"--{param_name}="):
            return arg.split("=")[1]
    return None

glb_location = get_param("glb_loc") or glb_location # path to where all .glb files are located, this folder should have all the glb files in the format scene_name.glb
csv_location = get_param("csv_loc") or csv_location # path to where all .csv files for camera parameters are located, this folder should include scene folder which should include the camera_poses.csv file
config_location = get_param("config_loc") or config_location #location of the config json file generated by import_prep script
output_location = get_param("output_loc") or output_location # output path

#invalid check if empty and exit
if not glb_location:
    print("Invalid GLB location")
    exit(1)
if not csv_location:
    print("Invalid CSV location")
    exit(1)
if not config_location:
    print("Invalid config location")
    exit(1)
if not output_location:
    print("Invalid output location")
    exit(1)

print(f"Config {config_location}")

if(os.path.exists(config_location)):
    print(f"Using config file: {config_location}")
    config = json.load(open(config_location))
else:
    print(f"Config file not found: {config_location}")
    config = {
        "data":[{
            "glb_location": glb_location,
            "csv_location": csv_location,
            "floors": floor_count,
            "id": os.path.basename(glb_location).split('.')[0]
        }]
    }

if "render_size" not in config:
    config["render_size"] = render_size

def process_scene(render_size, glb_location, csv_location,output_location,filename,floor_count):

    print(f"Processing {filename} with {floor_count} floors")

    extension = os.path.basename(glb_location).split('.')[1]

    bpy.ops.wm.read_factory_settings(use_empty=True)    

    bpy.context.scene.render.engine = 'BLENDER_EEVEE'

    bpy.ops.import_scene.gltf(filepath=glb_location)

    mesh_obj = bpy.data.objects[0]

    for obj in bpy.data.objects:
        if obj.type == 'MESH':
            mesh_obj = obj
            break

    mesh_obj.rotation_mode = "XYZ"
    mesh_obj.rotation_euler[0] -= math.pi/2  # -90 degrees in radians

    material = mesh_obj.active_material

    if material and material.use_nodes:
        nodes = material.node_tree.nodes
        links = material.node_tree.links

        # Find the Principled BSDF node
        principled_bsdf = None
        for node in nodes:
            if node.type == 'BSDF_PRINCIPLED':
                principled_bsdf = node
                break

        if principled_bsdf:
            # Find the texture node connected to the base color
            base_color_input = principled_bsdf.inputs['Base Color']
            if (base_color_input.is_linked):
                texture_node = base_color_input.links[0].from_node

                # Create an Emission shader node
                emission_node = nodes.new(type='ShaderNodeEmission')
                emission_node.location = principled_bsdf.location
                emission_node.location.x -= 200

                # Connect the texture node to the Emission shader node
                links.new(texture_node.outputs[0], emission_node.inputs[0])

                # Connect the Emission shader node to the Material Output
                material_output = nodes.get('Material Output')
                links.new(emission_node.outputs[0], material_output.inputs['Surface'])

    #Find the center of the mesh
    bbox_corners = [mesh_obj.matrix_world @ Vector(corner) for corner in mesh_obj.bound_box]
    bbox_center = sum(bbox_corners, Vector()) / 8 # average of 8 corners
    bbox_center.y, bbox_center.z = bbox_center.z, 10

    # Create camera
    camera_data = bpy.data.cameras.new(name='Camera')
    camera_object = bpy.data.objects.new('Camera', camera_data)
    bpy.context.collection.objects.link(camera_object)

    camera_data.type = 'ORTHO'

    camera_object.location = bbox_center
    camera_object.rotation_euler = (0, 0, 0)

    max_dimension = max(mesh_obj.dimensions)
    camera_data.ortho_scale = math.ceil(max_dimension)

    bpy.context.scene.camera = camera_object

    bpy.context.scene.render.resolution_x = bpy.context.scene.render.resolution_y = render_size

    meters_per_pixel = camera_data.ortho_scale / render_size
    pixels_per_meter = render_size / camera_data.ortho_scale

    camera_positions = []
    camera_rotations = []
    camera_names = []
    with open(csv_location, newline='') as csvfile:
        reader = csv.reader(csvfile)
        for row in reader:
            camera_names.append(row[0])
            camera_positions.append(list(map(float, row[1:4])))
            camera_rotations.append(list(map(float, row[4:8])))

    camera_positions_screenspace = []
    for pos in camera_positions:
        pos = list(pos)
        screen_pos = [(pos[i] - camera_object.location[i]) / meters_per_pixel for i in range(2)]
        x = round(screen_pos[0] + (render_size / 2))
        y = round(screen_pos[1] + (render_size / 2))
        camera_positions_screenspace.append((x, y))

    # Create json data  
    json_data = {
        "mesh": filename,
        "floor_count": floor_count,        
        "pixel_scale": pixels_per_meter,
        "cameras":[]
    }

    # Add cameras to json data
    for i, pos in enumerate(camera_positions):
        json_data["cameras"].append({
            "name": camera_names[i],
            "position": pos,
            "rotation": camera_rotations[i],
            "screen_position": camera_positions_screenspace[i],
            'floor': 0
        })

    output_path = os.path.join(output_location, f'{filename}_data.json')
    
    

    #Render images
    if(floor_count > 1):
        z_values = np.array([pos[2] for pos in camera_positions]).reshape(-1, 1)

        dbscan = DBSCAN(eps=0.5, min_samples=7).fit(z_values)
        labels = dbscan.labels_
        unique_labels = set(labels)
        floor_count_new = len(unique_labels) - (1 if -1 in labels else 0)  # Exclude noise if present

        if floor_count_new != floor_count:
            json_data["floor_count"] = floor_count_new
            json_data["message"] = f"DBSCAN detected different floor count before:{floor_count} after:{floor_count_new}"

        floor_count = floor_count_new

        centroids = {}
        for label in unique_labels:
            if label == -1:
                continue  # Skip noise
            label_positions = [pos[2] for pos, lbl in zip(camera_positions, labels) if lbl == label]
            centroid = np.mean(label_positions, axis=0)
            centroids[label] = centroid

        # Assign noise points to the closest centroid
        for i, label in enumerate(labels):
            if label == -1:  # Noise
                _distances = [abs(camera_positions[i][2] - centroid) for centroid in centroids.values()]
                closest_centroid_label = min(centroids.keys(), key=lambda k: abs(camera_positions[i][2] - centroids[k]))
                labels[i] = closest_centroid_label

        # Remap labels to ascending order based on centroid values
        sorted_centroids = sorted(centroids.items(), key=lambda x: x[1])
        new_label_map = {orig_label: new_idx for new_idx, (orig_label, _) in enumerate(sorted_centroids)}
        labels = [new_label_map[label] for label in labels]

        print(json_data["message"])

        # kmeans = KMeans(n_clusters=floor_count, random_state=0).fit(z_values) 

        # cluster_centers = []
        

        # cluster_centers = kmeans.cluster_centers_        
        # cluster_centers = sorted(cluster_centers.flatten())

        cluster_centers = centroids
        cluster_centers = sorted(cluster_centers.values())

        #cluster_centers = [centroids[i] for i in range(len(centroids))]

        print(cluster_centers)

        camera_height = next((center for center in cluster_centers if center > 0), 0)

        print(f"Camera height: {camera_height}")

        floor_roofs =[]

        floor_height = 3

        for floor in range(floor_count):
            print(f"Rendering floor {floor}")
            if(floor < floor_count-1):
                floor_height = abs(cluster_centers[floor]-cluster_centers[floor+1])
            floor_roof = cluster_centers[floor] + floor_height *0.5
            if(floor < floor_count-1):                
                floor_roof = min(floor_roof, cluster_centers[floor+1]- camera_height - floor_height * 0.1)
            camera_object.location.z = floor_roof
            camera_data.clip_start = 0.001
            camera_data.clip_end = floor_height * 1.1
            render_output_path = os.path.join(output_location, f"{filename}_floor{floor}.png")
            bpy.context.scene.render.image_settings.file_format = 'PNG'
            bpy.context.scene.render.filepath = render_output_path
            bpy.ops.render.render(write_still=True)
            floor_roofs.append(floor_roof)

        # Assign floors to cameras
        for i, pos in enumerate(camera_positions):
            pos[2] = camera_height
            #floor = kmeans.labels_[i]
            floor = labels[i]
            json_data["cameras"][i]['floor'] = int(floor)
            json_data["cameras"][i]['position'] = pos
    else:
        print("Rendering single floor")
        render_output_path = os.path.join(output_location, f"{filename}_floor0.png")
        bpy.context.scene.render.image_settings.file_format = 'PNG'
        bpy.context.scene.render.filepath = render_output_path
        bpy.ops.render.render(write_still=True)

    # Save json data
    with open(output_path, 'w') as json_file:
        json.dump(json_data, json_file, indent=4)

    #print(f"Images rendered")
    print(f"Data saved to {output_path}")

    #DEBUGGING CLUSTERING
    # if not bpy.app.background:
    #     for pos in camera_positions:
    #         bpy.ops.mesh.primitive_uv_sphere_add(radius=0.1, location=pos)   
    #         sphere = bpy.context.view_layer.objects.active    
    #         mat = bpy.data.materials.new(name="RedMaterial")
    #         mat.diffuse_color = (1, 0, 0, 1)  # Red color
    #         sphere.data.materials.append(mat)

    #     for center in cluster_centers:
    #         bpy.ops.mesh.primitive_uv_sphere_add(radius=0.2, location=(bbox_center.x, bbox_center.y, center))  
    #         sphere = bpy.context.view_layer.objects.active        
    #         mat = bpy.data.materials.new(name="GreenMaterial")
    #         mat.diffuse_color = (0, 1, 0, 1)  # Green color
    #         sphere.data.materials.append(mat)

    #     for center in floor_roofs:
    #         bpy.ops.mesh.primitive_uv_sphere_add(radius=0.1, location=(bbox_center.x, bbox_center.y, center))  
    #         sphere = bpy.context.view_layer.objects.active        
    #         mat = bpy.data.materials.new(name="BLUE")
    #         mat.diffuse_color = (0, 0, 1, 1)
    #         sphere.data.materials.append(mat)

for scene in config["data"]:
    render_size = config["render_size"]
    glb_location = scene["glb_location"]
    csv_location = scene["csv_location"]
    floor_count = scene["floors"]

    print(f"Processing {glb_location} with {csv_location} floors {scene['id']}")

    if(output_location == "_"):output_dir = os.path.dirname(glb_location)
    else:
        output_dir = os.path.join(output_location, scene['id'])
        if not os.path.exists(output_dir):
            os.makedirs(output_dir)
    
    #************* RUN THIS LINE TO PROCESS THE SCENE *************
    process_scene(render_size, glb_location, csv_location, output_dir, scene['id'], floor_count)

    # # Copy the directory /panp/rgb inside of csv location to output dir
    source_dir = os.path.join(os.path.dirname(csv_location), 'pano', 'rgb')
    destination_dir = os.path.join(output_dir, 'pano_rgb')

    if os.path.exists(source_dir) and not os.path.exists(destination_dir):
        shutil.copytree(source_dir, destination_dir)
        print(f"Copied {source_dir} to {destination_dir}")
    
    # #Rename files inside the rgb folder
    if os.path.exists(destination_dir):
        for filename in os.listdir(destination_dir):
            if filename.startswith('p'):
                new_name = filename.split('_')[1] + '.png'
                old_file = os.path.join(destination_dir, filename)
                new_file = os.path.join(destination_dir, new_name)
                os.rename(old_file, new_file)
        print(f"Panorama renaming done {len(os.listdir(destination_dir))}")    

    source_dir = os.path.join(os.path.dirname(csv_location), 'pano', 'mist')
    destination_dir = os.path.join(output_dir, 'pano_depth')

    if os.path.exists(source_dir) and not os.path.exists(destination_dir):
        shutil.copytree(source_dir, destination_dir)
        print(f"Copied {source_dir} to {destination_dir}")

    # Rename files inside the rgb folder
    if os.path.exists(destination_dir):
        for filename in os.listdir(destination_dir):
            if filename.startswith('p'):
                new_name = filename.split('_')[1] + '.png'
                old_file = os.path.join(destination_dir, filename)
                new_file = os.path.join(destination_dir, new_name)
                os.rename(old_file, new_file)
        print(f"Depth renaming done {len(os.listdir(destination_dir))}")
                
    